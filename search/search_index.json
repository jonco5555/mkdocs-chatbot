{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MkDocs Chatbot Plugin","text":""},{"location":"#overview","title":"Overview","text":"<p>Tired of endlessly scrolling through documentation to find what you need?</p> <p>Mkdocs-chatbot transforms your docs into an interactive, AI-powered chat experience \u2014 so you get instant answers, personalized guidance, and a smarter way to explore content. Say goodbye to frustration and hello to effortless discovery!</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the plugin:</p> uvpip <pre><code>uv add --group docs mkdocs-chatbot\n</code></pre> <pre><code>pip install mkdocs-chatbot\n</code></pre> <p>Add the plugin to your <code>mkdocs.yaml</code> configuration: <pre><code>plugins:\n    - chatbot:\n        url: &lt;your_chat_url&gt;\n</code></pre></p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>The plugin requires a backend chat application to function. The plugin itself only provides the frontend interface (chatbot button and iframe). You need to:</p> <ol> <li>Set up a backend chat service that embeds your documentation and connects to an LLM</li> <li>Configure the <code>url</code> parameter to point to your backend service</li> </ol> <p>For detailed information on backend requirements, setup instructions, and how to interface with the plugin, see the Backend Setup Guide.</p>"},{"location":"backend_setup/","title":"Backend Setup","text":""},{"location":"backend_setup/#backend-requirements","title":"Backend Requirements","text":"<p>The <code>mkdocs-chatbot</code> plugin provides the frontend interface (the chatbot button and iframe) that appears in your MkDocs documentation. However, to make the chatbot functional, you need to set up a separate backend chat application that:</p> <ol> <li>Embeds your documentation - Processes your Markdown files and creates vector embeddings for semantic search</li> <li>Provides a chat interface - Serves a web-based chat UI that can be embedded in an iframe</li> <li>Connects to an LLM - Uses a language model (like Google Gemini, OpenAI GPT, etc.) to generate responses based on your documentation</li> </ol>"},{"location":"backend_setup/#interface-requirements","title":"Interface Requirements","text":"<p>The backend chat application must:</p> <ul> <li>Be accessible via URL - The plugin loads the chat interface in an iframe, so your backend must be accessible at a URL (e.g., <code>http://localhost:8501</code> for local development or <code>https://your-chat-app.com</code> for production)</li> <li>Support iframe embedding - The chat interface should be designed to work within an iframe (typically 400px wide)</li> <li>Accept a project parameter (optional) - The plugin automatically appends <code>?project=&lt;project_name&gt;</code> to the URL, which can be used to support multiple documentation projects from a single backend</li> </ul>"},{"location":"backend_setup/#project-name-detection-and-url-construction","title":"Project Name Detection and URL Construction","text":"<p>The plugin automatically determines the project name and constructs the chatbot URL as follows:</p>"},{"location":"backend_setup/#project-name-detection","title":"Project Name Detection","text":"<p>The plugin extracts the project name from the current page URL:</p> <ul> <li>Local development (<code>localhost</code> or <code>127.0.0.1</code>): The project name is set to <code>\"default\"</code></li> <li>Production/other hosts: The project name is extracted from the first path segment of the URL</li> <li>For example, if the documentation is served at <code>https://docs.example.com/myproject/page1</code>, the project name will be <code>\"myproject\"</code></li> <li>If no path segment exists, it defaults to <code>\"default\"</code></li> </ul>"},{"location":"backend_setup/#url-construction","title":"URL Construction","text":"<p>The plugin constructs the chatbot iframe URL by:</p> <ol> <li>Taking the base <code>chatbot_url</code> from your configuration</li> <li>Checking if the URL already contains query parameters (by looking for a <code>?</code> character)</li> <li>Appending the project parameter:</li> <li>If the URL already has query parameters: <code>&amp;project=&lt;project_name&gt;</code></li> <li>If the URL has no query parameters: <code>?project=&lt;project_name&gt;</code></li> </ol>"},{"location":"example/","title":"Example","text":""},{"location":"example/#example-implementation","title":"Example Implementation","text":"<p>Under the <code>app</code> folder, you'll find a simple Streamlit application that demonstrates how to build a compatible backend:</p>"},{"location":"example/#features","title":"Features","text":"<ul> <li>Uses <code>llama-index</code> for document embedding and retrieval</li> <li>Integrates with Google Gemini for LLM responses</li> <li>Loads documentation from your <code>docs/</code> directory</li> <li>Provides a chat interface optimized for iframe embedding</li> </ul>"},{"location":"example/#setup-instructions","title":"Setup Instructions","text":"<ol> <li> <p>Install dependencies:    <pre><code>cd app\nuv sync  # or pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Configure API keys:</p> </li> <li>The example uses Google Gemini API</li> <li>Set your API key in <code>app/main.py</code> (or use environment variables for production)</li> <li> <p>You'll need:</p> <ul> <li>Google Gemini API key for the LLM</li> <li>Google Gemini API key for embeddings</li> </ul> </li> <li> <p>Configure the documentation path:</p> </li> <li>Update <code>DATA_PATH</code> in <code>app/main.py</code> to point to your documentation directory</li> <li> <p>By default, it points to <code>../docs/</code> relative to the app directory</p> </li> <li> <p>Run the Streamlit app:    <pre><code>streamlit run app/main.py\n</code></pre>    The app will typically run on <code>http://localhost:8501</code></p> </li> <li> <p>Configure MkDocs:    <pre><code>plugins:\n    - chatbot:\n        url: \"http://localhost:8501\"  # or your production URL\n</code></pre></p> </li> </ol>"},{"location":"example/#building-your-own-backend","title":"Building Your Own Backend","text":"<p>You're not limited to the Streamlit example. You can build your backend using any framework (Flask, FastAPI, Django, etc.) as long as it:</p> <ul> <li>Serves a web page that can be embedded in an iframe</li> <li>Provides a chat interface (can be simple HTML/JavaScript or a full framework)</li> <li>Connects to an embedding service and LLM API</li> <li>Processes your documentation files for semantic search</li> </ul> <p>The backend can be: - Hosted separately - Deploy your chat backend independently (e.g., on a cloud service) - Co-located with MkDocs - Run both services on the same server - Multi-tenant - Support multiple documentation projects using the <code>project</code> query parameter</p>"}]}